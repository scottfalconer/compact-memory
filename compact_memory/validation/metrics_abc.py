from __future__ import annotations

"""Abstract base class for validation metrics."""

from abc import ABC, abstractmethod
from typing import Any, Dict, Optional, TYPE_CHECKING

if TYPE_CHECKING:  # pragma: no cover - imports for type hints only
    from compact_memory.engines import CompressedMemory, CompressionTrace
else:  # pragma: no cover - avoid runtime dependency
    CompressedMemory = Any
    CompressionTrace = Any


class ValidationMetric(ABC):
    """
    Abstract Base Class for defining validation metrics.

    Validation metrics are used to evaluate the performance of compression
    strategies, the quality of LLM responses, or other aspects of the
    Compact Memory system. Developers can create custom metrics by subclassing
    this class and implementing the `evaluate` method.

    Each metric must also have a unique `metric_id` class attribute (a string)
    used for registration and selection within the framework.

    Metric-specific configuration parameters can be passed during instantiation
    via `**kwargs` and are stored in `self.config_params`.

    Example:
    ```python
    from compact_memory.validation.metrics_abc import ValidationMetric
    from compact_memory.engines import CompressedMemory, CompressionTrace
    from typing import Dict, Optional

    class MyCustomMetric(ValidationMetric):
        metric_id = "my_custom_metric"

        def __init__(self, custom_param: int = 5, **kwargs):
            super().__init__(**kwargs)
            self.custom_param = custom_param # Store specific params

        def evaluate(self, llm_response: str, reference_answer: str, **kwargs) -> Dict[str, float]:
            # ... implementation to calculate score(s) ...
            score = 0.0 # Placeholder
            return {"my_custom_score": score, "param_value": float(self.custom_param)}
    ```
    """

    metric_id: str  # Unique string identifier for the validation metric.

    def __init__(self, **kwargs: Any) -> None:  # pragma: no cover - simple init
        """
        Initializes the validation metric and stores configuration parameters.

        Args:
            **kwargs: Metric-specific configuration parameters. These are stored
                      in `self.config_params` and can be used by the `evaluate` method.
        """
        self.config_params = kwargs

    @abstractmethod
    def evaluate(
        self,
        llm_response: Optional[str] = None,
        reference_answer: Optional[str] = None,
        original_query: Optional[str] = None,
        compressed_context: Optional[CompressedMemory] = None,
        compression_trace: Optional[CompressionTrace] = None,
        llm_provider_info: Optional[Dict[str, Any]] = None,
        # Added original_text and compressed_text for direct compression evaluation
        original_text: Optional[str] = None,
        compressed_text: Optional[str] = None,
        **kwargs: Any,
    ) -> Dict[str, float]:
        """
        Evaluates a specific aspect and returns a dictionary of scores.

        This method must be implemented by concrete metric subclasses.
        The method is designed to be flexible and can evaluate different things
        based on the arguments provided. For example:
        - To evaluate LLM response quality: provide `llm_response` and `reference_answer`.
        - To evaluate compression effectiveness: provide `original_text` and `compressed_text`.
        - To evaluate retrieval quality (indirectly): `compressed_context` (if it contains retrieved items)
          and `original_query` / `reference_answer`.

        Subclasses should pick the arguments relevant to their specific calculation.

        Args:
            llm_response: The text generated by an LLM.
            reference_answer: The ground truth or target answer/text.
            original_query: The user query that led to the LLM response.
            compressed_context: The `CompressedMemory` object that was (or would be)
                                provided as context to the LLM.
            compression_trace: The `CompressionTrace` object from the compression
                               stage, if applicable.
            llm_provider_info: A dictionary containing information about the LLM
                               provider or model used (e.g., model name, API endpoint).
            original_text: The original, uncompressed text. Used for metrics
                           evaluating compression directly (e.g., compression ratio).
            compressed_text: The text after compression. Used for metrics
                             evaluating compression directly.
            **kwargs: Additional keyword arguments that specific metrics might require.

        Returns:
            A dictionary where keys are metric names (e.g., "rougeL",
            "compression_ratio", "bleu") and values are the corresponding
            float scores.
        """
