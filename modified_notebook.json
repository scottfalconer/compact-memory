{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Compact Memory User Guide\n",
    "\n",
    "Welcome to the `compact-memory` user guide! This notebook provides a hands-on introduction to the `compact-memory` library, focusing on its command-line interface. This version includes packaging for simplified usage.\\n",
    "\n",
    "`compact-memory` helps you manage and compress text data (like dialogue history or large documents) to fit within the limited context windows of Large Language Models (LLMs) while retaining essential information. This guide focuses on using the `compact-memory` library.\\n",
    "\n",
    "This notebook will walk you through:\n",
    "1.  **Setup**: Cloning the `compact-memory` repository, changing into its directory, and installing the package along with dependencies.\\n",
    "2.  **Command Line Interface (CLI)**: A tour of the `compact-memory` CLI for compressing text, trying different strategies, and interacting with an LLM.\n",
    "3.  **Conclusion**: Summary and next steps for using `compact-memory`.\n",
    "\n",
    "**Important Note on Notebook Execution**: This notebook uses `%cd` to change the current directory. Subsequent cells will operate from within the cloned `compact-memory` repository root.\n",
    "\n",
    "Let's get started!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Initial Setup: Install Dependencies\n",
    "\n",
    "First, we need to set up the environment. This involves cloning the `compact-memory` repository, changing our current directory into it, installing the package from local source, and then downloading necessary dependencies and models.\\n",
    "\n",
    "**Note**: If you are running this notebook locally and have already cloned this repository and set up a virtual environment from within the repo root, you might be able to skip some of these steps. However, these cells are designed to work in a fresh Google Colab environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Clone `compact-memory` Repository and Change Directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the compact-memory repository\n",
    "!git clone https://github.com/scottfalconer/compact-memory.git\n",
    "\n",
    "# Change directory into the cloned repository\n",
    "# Subsequent commands will run from the root of the 'compact-memory' repository\n",
    "%cd compact-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we are inside the `compact-memory` directory (from the `scottfalconer/compact-memory` repository), we can install the package and its dependencies using relative paths."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Install `compact-memory` Package from Local Source"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the compact-memory package from the local source (current directory)\n",
    "# The --no-build-isolation flag can be helpful if there are issues with \n",
    "# the build environment or specific package versions.\n",
    "!pip install . --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Download spaCy Model\n",
    "\n",
    "Download the English language model from spaCy, which is used for text processing tasks like sentence segmentation. SpaCy should have been installed as part of the `requirements.txt`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Set up PYTHONPATH (Usually Not Needed with `pip install .`)\n",
    "\n",
    "When a package is installed using `pip install .` from its root directory, it's typically placed in the Python environment's `site-packages` directory and becomes accessible. Explicitly modifying `sys.path` is usually not necessary.\n",
    "\n",
    "The following cell is commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.getcwd()) # os.getcwd() is now the repo root, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Download Pre-trained Models for `compact-memory`\n",
    "\n",
    "Download the default embedding model (for text vectorization) and a small chat model (for response generation experiments) using the `compact-memory` CLI. The CLI should be available after the `pip install .` step.\\n",
    "\\n",
    "For the chat model, this guide uses `gpt2`, a smaller model suitable for local demonstrations without requiring API keys. The library can be configured to use other models, including more powerful ones like the default `openai/gpt-3.5-turbo`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the default sentence transformer model for embeddings\n",
    "!compact-memory dev download-embedding-model --model-name all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small chat model for demonstration purposes\n",
    "!compact-memory dev download-chat-model --model-name gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7 Configure Offline Usage (Optional)\n",
    "\n",
    "If you have downloaded all necessary models and want to ensure the notebook runs without attempting to access Hugging Face Hub, you can set these environment variables. For this showcase, we'll leave them commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For offline use after all models are downloaded, uncomment the following lines:\n",
    "# import os\n",
    "# os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "# os.environ['TRANSFORMERS_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Command Line Interface (CLI) Showcase\n",
    "\n",
    "`compact-memory` also features a versatile Command Line Interface (CLI) for performing common operations without needing to write Python scripts. This is handy for quick tests, batch processing, model downloads, and direct interaction with compression strategies. The CLI became available after we installed `compact-memory` using `pip`.\n",
    "\n",
    "Since we are operating from within the `compact-memory` repository root, file paths for the CLI should also be relative to this root."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Basic Help Command\n",
    "\n",
    "To view all available CLI commands and their general options, use the `--help` flag."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory --help"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Compression Command (`compress`)\n",
    "\n",
    "The `compress` command lets you apply a compression engine to various forms of input: a direct line of text (using `--text`), or a single file or an entire directory (by providing the path as a positional argument). The following subsections will demonstrate these use cases.\n",
    "\n",
    "The CLI uses strategies that are built into `compact-memory` (e.g., `gist`, `truncate`) or can be extended with plugins. For these examples, we'll use some of the built-in strategies."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compress a short text string using the 'truncate' engine (if available as a built-in/plugin)\n",
    "# This command demonstrates compressing a simple line of text. \n",
    "# We use the 'truncate' engine to keep the beginning of the text, fitting it within a 20-token budget.\n",
    "!compact-memory compress --engine truncate --text \"This is a fairly long sentence that we want to compress using the command line interface to a small number of tokens.\" --budget 20\n",
    "\n",
    "# Example: Compress a text file using the 'gist' engine\n",
    "# This command demonstrates compressing an entire file. We use 'sample_data/moon_landing/full.txt'.\n",
    "# The 'gist' engine is employed to summarize the content within a 100-token budget.\n",
    "# To compress a file, provide its path as an argument. For instance:\n",
    "!compact-memory compress --engine gist sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Compress an entire directory using the 'gist' engine\n",
    "# This command demonstrates compressing all supported files within a directory.\n",
    "# We use the 'sample_data/moon_landing' directory.\n",
    "# The 'gist' engine will be applied to each file, and the overall output might be a concatenation or structured representation.\n",
    "# The `compress` command automatically detects that this is a directory and processes its contents.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compress an entire directory using the 'gist' engine\n",
    "# This command demonstrates compressing all supported files within a directory.\n",
    "# We use the 'sample_data/moon_landing' directory.\n",
    "# The 'gist' engine will be applied to each file.\n",
    "# The `compress` command directly handles directory paths.\n",
    "!compact-memory compress --engine gist sample_data/moon_landing --budget 200\n",
    "# You might want to add --output some_directory_output.json if you want to inspect results later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a Engine with `--engine`\n",
    "\n",
    "The `--engine` option is key to controlling how `compact-memory` compresses your text. Different strategies have different behaviors:\n",
    "\n",
    "*   **`gist`**: Aims to create a concise summary or gist of the text. (Already used in file/directory examples above).\n",
    "*   **`truncate`**: Simply cuts the text to fit the token budget, usually keeping the beginning. (Already used in the text line example above).\n",
    "*   **`passthrough`** (if available, or another simple built-in one): Might do minimal or no actual compression, useful for just token counting or as a baseline.\n",
    "\n",
    "You can switch between them easily. For example, to compress our `full.txt` using `truncate` instead of `gist`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using a different engine (truncate) on the full.txt file\n",
    "# To compress a file, provide its path as an argument. For instance:\n",
    "!compact-memory compress --engine truncate sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Common options for `compact-memory compress`:\n",
    "-   `--engine <name>`: ID of the compression engine (e.g., `gist`, `truncate`).\n",
    "-   `--text \"<string>\"`: The text string to compress.\n",
    "-   `<path_to_file_or_dir>`: (Positional) Path to the text file or directory to compress when `--text` is not used.\n",
    "-   `--budget <int>`: The target token budget.\n",
    "-   `--tokenizer <name>`: (Optional) Specify a tokenizer if the engine shouldn't use its default.\n",
    "-   `--output <path>`: (Optional) Path to save the compressed output (e.g., as JSON). The compressed text is typically printed to the standard output if `--output` is not specified. Using `--output` is recommended for saving results, especially for larger inputs or when integrating into scripts.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Talk Command (`talk`)\n",
    "\n",
    "The `talk` command enables interactive chat with an LLM. To provide the LLM with a condensed background from a document or previous interactions, it's best to first compress this information and then load it into the conversation.\n",
    "\n",
    "The recommended way to do this is:\n",
    "1. Compress your context (e.g., a document or chat history) using `compact-memory compress ... --output context_memory.json`.\n",
    "2. Start your conversation using this pre-compressed context with `compact-memory talk --memory context_memory.json ...`.\n",
    "\n",
    "Running a fully interactive `talk` session is not feasible within a static notebook cell. Instead, we'll show how to get help for the command and provide conceptual examples of how you would use it in your terminal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory talk --help"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conceptual Usage of `talk` (run in your terminal):**\n",
    "\n",
    "1.  **Chat with a pre-compressed document as context:**\n",
    "    First, compress your document:\n",
    "    ```bash\n",
    "    # (Ensure you are in the 'compact-memory' directory if using relative paths like below)\n",
    "    compact-memory compress --engine gist ./sample_data/moon_landing/full.txt --budget 200 --output moon_context.json\n",
    "    ```\n",
    "    Then, start an interactive chat session using this compressed document as memory. You can optionally start with a message:\n",
    "    ```bash\n",
    "    compact-memory talk --memory moon_context.json --chat-model gpt2 --message \"What was the primary goal of the mission described in the document?\"\n",
    "    ```\n",
    "\n",
    "2.  **Chat with custom initial text as context:**\n",
    "    First, create a file with your text (e.g., `my_initial_notes.txt`):\n",
    "    ```bash\n",
    "    echo \"Key points for discussion: AI ethics, data privacy, and future of work.\" > my_initial_notes.txt\n",
    "    ```\n",
    "    Next, compress these notes:\n",
    "    ```bash\n",
    "    compact-memory compress --engine gist my_initial_notes.txt --budget 50 --output notes_memory.json\n",
    "    ```\n",
    "    Finally, start your chat using these compressed notes:\n",
    "    ```bash\n",
    "    compact-memory talk --memory notes_memory.json --chat-model gpt2\n",
    "    ```\n",
    "\n",
    "For a quick, non-interactive check within this notebook, you can try sending a single message. The output will be the LLM's response (or an indication it's meant for interactive use):\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This attempts a non-interactive query with a specified chat model.\n",
    "# The CLI might provide a single response or indicate it's for interactive use.\n",
    "!compact-memory talk --chat-model gpt2 --message \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Other Useful CLI Commands\n",
    "\n",
    "The `compact-memory` CLI offers other utilities. Here are a few, with examples of how to get their help messages:\n",
    "\n",
    "-   `download-model`, `download-chat-model`: We used these in the setup to fetch models (e.g., `all-MiniLM-L6-v2`, `gpt2`).\\n",
    "-   `engine stats`: Provides statistics about the engine store's storage.\\n",
    "-   `engine validate`: Validates the integrity of the engine store's storage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory engine stats --help\\n",
    "!echo \"\\n---------------------------------------\\n\" # Visual separator\n",
    "!compact-memory engine validate --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion and Next Steps\n",
    "\n",
    "This notebook has provided a user-focused introduction to the `compact-memory` library, specifically covering:\\n",
    "-   **Installation and Setup**: How to get the library ready for use.\n",
    "-   **Command Line Interface (CLI)**: Practical examples of using the CLI to compress text, try different strategies, and provide context to an LLM with the `talk` command.\n",
    "\n",
    "With `compact-memory`, you can effectively manage text data to fit within LLM context windows. This is particularly useful for tasks like:\n",
    "-   Summarizing long documents before feeding them to an LLM.\n",
    "-   Maintaining context in extended conversations with chatbots.\n",
    "-   Processing large amounts of text for information retrieval when interacting with LLMs.\n",
    "\n",
    "We encourage you to experiment with the CLI commands and strategies shown in this guide using your own data.\n",
    "\n",
    "### Further Resources:\n",
    "-   **GitHub Repository**: [https://github.com/google/compact-memory](https://github.com/google/compact-memory)\\n",
    "-   **CLI Help**: Use `compact-memory --help`, `compact-memory compress --help`, and `compact-memory talk --help` in your terminal for detailed CLI options.\n",
    "-   **Examples Folder**: Check the `examples/` directory in the repository for any other user-oriented examples or showcases that may be added.\n",
    "\n",
    "Happy compressing!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "compact_memory_user_guide.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}