{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Compact Memory User Guide\n",
    "\n",
    "Welcome to the `compact-memory` user guide! This notebook provides a hands-on introduction to the `compact-memory` library, focusing on its command-line interface.\n",
    "\n",
    "`compact-memory` helps you manage and compress text data (like dialogue history or large documents) to fit within the limited context windows of Large Language Models (LLMs) while retaining essential information. This guide focuses on using the `compact-memory` library.\n",
    "\n",
    "This notebook will walk you through:\n",
    "1.  **Setup**: Cloning the `compact-memory` repository, changing into its directory, and installing the package along with dependencies.\n",
    "2.  **Command Line Interface (CLI)**: A tour of the `compact-memory` CLI for compressing text, trying different strategies, and interacting with an LLM.\n",
    "3.  **Conclusion**: Summary and next steps for using `compact-memory`.\n",
    "\n",
    "**Important Note on Notebook Execution**: This notebook uses `%cd` to change the current directory. Subsequent cells will operate from within the cloned `compact-memory` repository root.\n",
    "\n",
    "Let's get started!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Initial Setup: Install Dependencies\n",
    "\n",
    "First, we need to set up the environment. This involves cloning the `compact-memory` repository, changing our current directory into it, installing the package from local source, and then downloading necessary dependencies and models.\n",
    "\n",
    "**Note**: If you are running this notebook locally and have already cloned this repository and set up a virtual environment from within the repo root, you might be able to skip some of these steps. However, these cells are designed to work in a fresh Google Colab environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Clone `compact-memory` Repository and Change Directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the compact-memory repository\n",
    "!git clone https://github.com/scottfalconer/compact-memory.git\n",
    "\n",
    "# Change directory into the cloned repository\n",
    "# Subsequent commands will run from the root of the 'compact-memory' repository\n",
    "%cd compact-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we are inside the `compact-memory` directory, we can install the package and its dependencies using relative paths."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Install `compact-memory` Package from Local Source"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the compact-memory package from the local source (current directory)\n",
    "# The --no-build-isolation flag can be helpful if there are issues with \n",
    "# the build environment or specific package versions.\n",
    "!pip install . --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Download spaCy Model\n",
    "\n",
    "Download the English language model from spaCy, which is used for text processing tasks like sentence segmentation. SpaCy should have been installed as part of the `requirements.txt`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Set up PYTHONPATH (Usually Not Needed with `pip install .`)\n",
    "\n",
    "When a package is installed using `pip install .` from its root directory, it's typically placed in the Python environment's `site-packages` directory and becomes accessible. Explicitly modifying `sys.path` is usually not necessary.\n",
    "\n",
    "The following cell is commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.getcwd()) # os.getcwd() is now the repo root, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Download Pre-trained Models for `compact-memory`\n",
    "\n",
    "Download the default embedding model (for text vectorization) and a small chat model (for response generation experiments) using the `compact-memory` CLI. The CLI should be available after the `pip install .` step.\n",
    "\n",
    "For the chat model, this guide uses `gpt2`, a smaller model suitable for local demonstrations without requiring API keys. The library can be configured to use other models, including more powerful ones like the default `openai/gpt-3.5-turbo`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the default sentence transformer model for embeddings\n",
    "!compact-memory dev download-embedding-model --model-name all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small chat model for demonstration purposes\n",
    "!compact-memory dev download-chat-model --model-name gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7 Configure Offline Usage (Optional)\n",
    "\n",
    "If you have downloaded all necessary models and want to ensure the notebook runs without attempting to access Hugging Face Hub, you can set these environment variables. For this showcase, we'll leave them commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For offline use after all models are downloaded, uncomment the following lines:\n",
    "# import os\n",
    "# os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "# os.environ['TRANSFORMERS_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Command Line Interface (CLI) Showcase\n",
    "\n",
    "`compact-memory` also features a versatile Command Line Interface (CLI) for performing common operations without needing to write Python scripts. This is handy for quick tests, batch processing, model downloads, and direct interaction with compression strategies. The CLI became available after we installed `compact-memory` using `pip`.\n",
    "\n",
    "Since we are operating from within the `compact-memory` repository root, file paths for the CLI should also be relative to this root."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Basic Help Command\n",
    "\n",
    "To view all available CLI commands and their general options, use the `--help` flag."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory --help"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Compression Command (`compress`)\n",
    "\n",
    "The `compress` command lets you apply a compression engine to various forms of input: a direct line of text (using `--text`), or a single file or an entire directory (by providing the path as a positional argument). The following subsections will demonstrate these use cases.\n",
    "\n",
    "The CLI uses strategies that are built into `compact-memory` (e.g., `pipeline`, `none`, `first_last`, `read_agent_gist`, `stopword_pruner`). For these examples, we'll use some of these built-in strategies. (Note: The 'prototype' engine was mentioned in a previous version of this guide but is not available in the current CLI; 'first_last' is used in its place in the examples below.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compress a short text string using the 'first_last' engine (Note: 'prototype' was previously mentioned but is not available in this environment)\n",
    "# This command demonstrates compressing a simple line of text. \n",
    "# We use the 'first_last' engine, fitting it within a 20-token budget.\n",
    "!compact-memory compress --engine first_last --text \"This is a fairly long sentence that we want to compress using the command line interface to a small number of tokens.\" --budget 20\n",
    "\n",
    "# Example: Compress a text file using the 'first_last' engine (Note: 'prototype' was previously mentioned but is not available in this environment)\n",
    "# This command demonstrates compressing an entire file. We use 'sample_data/moon_landing/full.txt'.\n",
    "# The 'first_last' engine is employed to summarize the content within a 100-token budget.\n",
    "# To compress a file, provide its path as an argument using --file. For instance:\n",
    "!compact-memory compress --engine first_last --file sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Compress an entire directory using the 'gist' engine\n",
    "# This command demonstrates compressing all supported files within a directory.\n",
    "# We use the 'sample_data/moon_landing' directory.\n",
    "# The 'gist' engine will be applied to each file, and the overall output might be a concatenation or structured representation.\n",
    "# The `compress` command automatically detects that this is a directory and processes its contents.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compress an entire directory using the 'first_last' engine (Note: 'prototype' was previously mentioned but is not available in this environment)\n",
    "# This command demonstrates compressing all supported files within a directory.\n",
    "# We use the 'sample_data/moon_landing' directory.\n",
    "# The 'first_last' engine will be applied to each file.\n",
    "# The `compress` command uses --dir for directory paths.\n",
    "!compact-memory compress --engine first_last --dir sample_data/moon_landing --budget 200\n",
    "# You might want to add --output some_directory_output.json if you want to inspect results later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a Engine with `--engine`\n",
    "\n",
    "The `--engine` option is key to controlling how `compact-memory` compresses your text. Different strategies have different behaviors. The available one-shot engines currently include `first_last`, `none`, `pipeline`, `read_agent_gist`, and `stopword_pruner`.\n",
    "\n",
    "*   **`first_last`**: A simple engine that typically keeps the first and last parts of the text. (Used in examples above as a substitute for the previously mentioned 'prototype' engine).\n",
    "*   **`none`**: Passes through the text without compression (useful for baseline or token counting).\n",
    "*   **`pipeline`**: Allows chaining multiple processing steps (more advanced and requires specific configuration not covered in this basic example).\n",
    "*   **`read_agent_gist`**: Available engine.\n",
    "*   **`stopword_pruner`**: Removes filler words and stopwords to minimize length while preserving key content.\n",
    "\n",
    "You can switch between them easily. For example, to process our `full.txt` using `none` (no compression) instead of `first_last` (which replaced `prototype`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using a different engine (none) on the full.txt file\n",
    "# To process a file, provide its path as an argument using --file. For instance:\n",
    "!compact-memory compress --engine none --file sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Common options for `compact-memory compress`:\n",
    "-   `--engine <name>`: ID of the compression engine (e.g., `prototype`, `pipeline`).\n",
    "-   `--text \"<string>\"`: The text string to compress.\n",
    "-   `<path_to_file_or_dir>`: (Positional) Path to the text file or directory to compress when `--text` is not used.\n",
    "-   `--budget <int>`: The target token budget.\n",
    "-   `--tokenizer <name>`: (Optional) Specify a tokenizer if the engine shouldn't use its default.\n",
    "-   `--output <path>`: (Optional) Path to save the compressed output (e.g., as JSON). The compressed text is typically printed to the standard output if `--output` is not specified. Using `--output` is recommended for saving results, especially for larger inputs or when integrating into scripts.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Other Useful CLI Commands\n",
    "\n",
    "The `compact-memory` CLI offers other utilities. Here are a few, with examples of how to get their help messages:\n",
    "\n",
    "-   `download-model`, `download-chat-model`: We used these in the setup to fetch models (e.g., `all-MiniLM-L6-v2`, `gpt2`).\n",
    "-   `engine stats`: Provides statistics about the engine store's storage.\n",
    "-   `engine validate`: Validates the integrity of the engine store's storage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory engine stats --help\n",
    "!echo \"\\n---------------------------------------\\n\" # Visual separator\n",
    "!compact-memory engine validate --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion and Next Steps\n",
    "\n",
    "This notebook has provided a user-focused introduction to the `compact-memory` library, specifically covering:\n",
    "-   **Installation and Setup**: How to get the library ready for use.\n",
    "-   **Command Line Interface (CLI)**: Practical examples of using the CLI to compress text, try different strategies, and provide context to an LLM with the `talk` command.\n",
    "\n",
    "With `compact-memory`, you can effectively manage text data to fit within LLM context windows. This is particularly useful for tasks like:\n",
    "-   Summarizing long documents before feeding them to an LLM.\n",
    "-   Maintaining context in extended conversations with chatbots.\n",
    "-   Processing large amounts of text for information retrieval when interacting with LLMs.\n",
    "\n",
    "We encourage you to experiment with the CLI commands and strategies shown in this guide using your own data.\n",
    "\n",
    "### Further Resources:\n",
    "-   **GitHub Repository**: [https://github.com/google/compact-memory](https://github.com/google/compact-memory)\n",
    "-   CLI Help: Use the help commands in your terminal.\n",
    "-   **Examples Folder**: Check the `examples/` directory in the repository for any other user-oriented examples or showcases that may be added.\n",
    "\n",
    "Happy compressing!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "compact_memory_user_guide.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
