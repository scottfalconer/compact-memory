{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Compact Memory Researcher Guide\n",
    "\n",
    "Welcome to the `compact-memory` researcher guide! This notebook provides a hands-on introduction to the `compact-memory` library (repository: `scottfalconer/compact-memory`), covering both command-line and Python interfaces. This version includes packaging for simplified usage.\n",
    "\n",
    "`compact-memory` is a library designed to help manage and compress text data (like dialogue history or large documents) to fit within the limited context windows of LLMs while retaining essential information. This guide focuses on the `scottfalconer/compact-memory` repository, which includes packaging fixes for easier setup and use.\n",
    "\n",
    "This notebook will walk you through:\n",
    "1.  **Setup**: Cloning the `compact-memory` repository (`scottfalconer/compact-memory`), changing into its directory, and installing the package along with dependencies.\n",
    "2.  **Command Line Interface (CLI)**: Leveraging the CLI for quick experiments, data preprocessing, and validation.\n",
    "3.  **Advanced Usage: Python Interface**: Diving deep into the Python API for defining custom compression strategies, configuring and running ingestion, history retrieval, and response generation experiments, and understanding the core components of the library.\n",
    "4.  **Conclusion**: Discussing research opportunities and how to contribute to `compact-memory`.\n",
    "\n",
    "**Important Note on Notebook Execution**: This notebook uses `%cd` to change the current directory. Subsequent cells will operate from within the cloned `compact-memory` repository root.\n",
    "\n",
    "Let's get started!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Initial Setup: Install Dependencies\n",
    "\n",
    "First, we need to set up the environment. This involves cloning the `compact-memory` repository (`scottfalconer/compact-memory`), changing our current directory into it, installing the package from local source, and then downloading necessary dependencies and models.\n",
    "\n",
    "**Note**: If you are running this notebook locally and have already cloned this specific repository and set up a virtual environment from within the repo root, you might be able to skip some of these steps. However, these cells are designed to work in a fresh Google Colab environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Clone `compact-memory` Repository and Change Directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the compact-memory repository\n",
    "!git clone https://github.com/scottfalconer/compact-memory.git\n",
    "\n",
    "# Change directory into the cloned repository\n",
    "# Subsequent commands will run from the root of the 'compact-memory' repository\n",
    "%cd compact-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we are inside the `compact-memory` directory (from the `scottfalconer/compact-memory` repository), we can install the package and its dependencies using relative paths."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Install `compact-memory` Package from Local Source"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the compact-memory package from the local source (current directory)\n",
    "# The --no-build-isolation flag can be helpful if there are issues with \n",
    "# the build environment or specific package versions.\n",
    "!pip install . --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Install Dependencies from `requirements.txt`\n",
    "\n",
    "Install the remaining dependencies listed in `requirements.txt` from the current directory. This ensures all features, including those for development and testing shown in this notebook, are available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Download spaCy Model\n",
    "\n",
    "Download the English language model from spaCy, which is used for text processing tasks like sentence segmentation. SpaCy should have been installed as part of the `requirements.txt`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Set up PYTHONPATH (Usually Not Needed with `pip install .`)\n",
    "\n",
    "When a package is installed using `pip install .` from its root directory, it's typically placed in the Python environment's `site-packages` directory and becomes accessible. Explicitly modifying `sys.path` is usually not necessary.\n",
    "\n",
    "The following cell is commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.getcwd()) # os.getcwd() is now the repo root, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Download Pre-trained Models for `compact-memory`\n",
    "\n",
    "Download the default embedding model (for text vectorization) and a small chat model (for response generation experiments) using the `compact-memory` CLI. The CLI should be available after the `pip install .` step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the default sentence transformer model for embeddings\n",
    "!compact-memory dev download-embedding-model --model-name all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small chat model for demonstration purposes\n",
    "!compact-memory dev download-chat-model --model-name gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7 Configure Offline Usage (Optional)\n",
    "\n",
    "If you have downloaded all necessary models and want to ensure the notebook runs without attempting to access Hugging Face Hub, you can set these environment variables. For this showcase, we'll leave them commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For offline use after all models are downloaded, uncomment the following lines:\n",
    "# import os\n",
    "# os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "# os.environ['TRANSFORMERS_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Command Line Interface (CLI) Showcase\n",
    "\n",
    "`compact-memory` also features a versatile Command Line Interface (CLI) for performing common operations without needing to write Python scripts. This is handy for quick tests, batch processing, model downloads, and direct interaction with compression strategies. The CLI became available after we installed `compact-memory` using `pip`.\n",
    "\n",
    "Since we are operating from within the `compact-memory` repository root, file paths for the CLI should also be relative to this root."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Basic Help Command\n",
    "\n",
    "To view all available CLI commands and their general options, use the `--help` flag."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory --help"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Compression Command (`compress`)\n",
    "\n",
    "The `compress` command lets you apply a compression strategy to various forms of input: a direct line of text (using `--text`), or a single file or an entire directory (by providing the path as a positional argument). This can be useful for quick validation of ideas, batch processing of datasets, or preparing data for more complex experiments. The following subsections will demonstrate these use cases.\n",
    "\n",
    "**Note on Custom Strategies and CLI**: Strategies defined in Python (like the `TruncateStrategy` later in this notebook) are available to the experiment framework. To make them accessible to the standalone CLI, they generally need to be part of the installed package or registered via a plugin mechanism. For CLI examples here, we primarily use built-in strategies like `gist` or `truncate`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compress a short text string using the 'truncate' strategy (if available as a built-in/plugin)\n",
    "# This command demonstrates compressing a simple line of text. \n",
    "# We use the 'truncate' strategy to keep the beginning of the text, fitting it within a 20-token budget.\n",
    "!compact-memory compress --strategy truncate --text \"This is a fairly long sentence that we want to compress using the command line interface to a small number of tokens.\" --budget 20\n",
    "\n",
    "# Example: Compress a text file using the 'gist' strategy\n",
    "# This command demonstrates compressing an entire file. We use 'sample_data/moon_landing/full.txt'.\n",
    "# The 'gist' strategy is employed to summarize the content within a 100-token budget.\n",
    "# To compress a file, provide its path as an argument. For instance:\n",
    "!compact-memory compress --strategy gist sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Compress an entire directory using the 'gist' strategy\n",
    "# This command demonstrates compressing all supported files within a directory.\n",
    "# We use the 'sample_data/moon_landing' directory.\n",
    "# The 'gist' strategy will be applied to each file, and the overall output might be a concatenation or structured representation.\n",
    "# The `compress` command automatically detects that this is a directory and processes its contents.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compress an entire directory using the 'gist' strategy\n",
    "# This command demonstrates compressing all supported files within a directory.\n",
    "# We use the 'sample_data/moon_landing' directory.\n",
    "# The 'gist' strategy will be applied to each file.\n",
    "# The `compress` command directly handles directory paths.\n",
    "!compact-memory compress --strategy gist sample_data/moon_landing --budget 200\n",
    "# You might want to add --output some_directory_output.json if you want to inspect results later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a Strategy with `--strategy`\n",
    "\n",
    "The `--strategy` option is key to controlling how `compact-memory` compresses your text. Different strategies have different behaviors:\n",
    "\n",
    "*   **`gist`**: Aims to create a concise summary or gist of the text. (Already used in file/directory examples above).\n",
    "*   **`truncate`**: Simply cuts the text to fit the token budget, usually keeping the beginning. (Already used in the text line example above).\n",
    "*   **`passthrough`** (if available, or another simple built-in one): Might do minimal or no actual compression, useful for just token counting or as a baseline.\n",
    "\n",
    "You can switch between them easily. For example, to compress our `full.txt` using `truncate` instead of `gist`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using a different strategy (truncate) on the full.txt file\n",
    "# To compress a file, provide its path as an argument. For instance:\n",
    "!compact-memory compress --strategy truncate sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Common options for `compact-memory compress`:\n",
    "-   `--strategy <name>`: ID of the compression strategy (e.g., `gist`, `truncate`).\n",
    "-   `--text \"<string>\"`: The text string to compress.\n",
    "-   `<path_to_file_or_dir>`: (Positional) Path to the text file or directory to compress when `--text` is not used.\n",
    "-   `--budget <int>`: The target token budget.\n",
    "-   `--tokenizer <name>`: (Optional) Specify a tokenizer if the strategy shouldn't use its default.\n",
    "-   `--output <path>`: (Optional) Path to save the compressed output (e.g., as JSON). The compressed text is typically printed to the standard output if `--output` is not specified. Using `--output` is recommended for saving results, especially for larger inputs or when integrating into scripts.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Talk Command (`talk`)\n",
    "\n",
    "The `talk` command enables interactive chat with an LLM. For research purposes, this can be used for qualitative assessment of how different memory compression strategies affect interaction quality. While some systems might attempt live conversation compression, `compact-memory` emphasizes a more robust approach for providing initial context: pre-compressing existing text (documents, prior dialogue) into a memory file.\n",
    "\n",
    "The recommended way to use `talk` with substantial context is to:\n",
    "1.  Compress the desired context (e.g., a background document, a lengthy chat history) using `compact-memory compress --output <output_file.json> ...`.\n",
    "2.  Load this pre-compressed context using `compact-memory talk --memory <output_file.json> ...`.\n",
    "\n",
    "This method ensures that the context provided to the LLM at the start of the `talk` session is already condensed by your chosen strategy. Running a fully interactive `talk` session is not feasible within a static notebook cell, so we'll show the help command and conceptual examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory talk --help"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conceptual Usage of `talk`:**\n",
    "\n",
    "You would typically run these in your terminal (from the repo root):\n",
    "\n",
    "1.  **Talk with pre-compressed initial context:**\n",
    "    ```bash\n",
    "    # 1. First, create some initial context and compress it:\n",
    "    echo \"This is some initial context for our conversation.\" > my_context.txt\n",
    "    compact-memory compress --strategy gist my_context.txt --budget 100 --output initial_memory.json\n",
    "    #\n",
    "    # 2. Then, start the conversation using this pre-compressed memory:\n",
    "    compact-memory talk --memory initial_memory.json --chat-model gpt2\n",
    "    ```\n",
    "    This multi-step process first compresses your desired context and then starts an interactive session with `gpt2` using that context.\n",
    "\n",
    "2.  **Talk using a pre-compressed memory file (e.g., a document):**\n",
    "    First, compress a document (using path relative to repo root):\n",
    "    ```bash\n",
    "    compact-memory compress --strategy gist ./sample_data/moon_landing/full.txt --budget 200 --output moon_memory.json\n",
    "    ```\n",
    "    Then, start a conversation using this memory as initial context, optionally with an initial message:\n",
    "    ```bash\n",
    "    compact-memory talk --memory moon_memory.json --chat-model gpt2 --message \"What were the main objectives?\"\n",
    "    ```\n",
    "\n",
    "For a non-interactive check within the notebook, you can try sending a single message. The behavior might vary:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This attempts a non-interactive query with a specified chat model.\n",
    "# The CLI might provide a single response or indicate it's for interactive use.\n",
    "!compact-memory talk --chat-model gpt2 --message \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Other Useful CLI Commands\n",
    "\n",
    "The `compact-memory` CLI offers other utilities. Here are a few, with examples of how to get their help messages:\n",
    "\n",
    "-   `download-model`, `download-chat-model`: We used these in the setup to fetch models (e.g., `all-MiniLM-L6-v2`, `tiny-gpt2`).\n",
    "-   `stats`: Provides statistics about datasets or memory files.\n",
    "-   `validate`: Validates configuration files or memory formats against the library's schemas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory agent stats --help\n",
    "!echo \"\\n---------------------------------------\\n\" # Visual separator\n",
    "!compact-memory agent validate --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Usage: Python Interface\n",
    "\n",
    "After exploring the CLI, this section delves into using `compact-memory` programmatically with Python for more advanced customization and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Importing Modules for Python Usage\n",
    "\n",
    "While the Command Line Interface (CLI) is excellent for quick tests, batch processing, and applying standard strategies, the Python interface is essential for advanced research. It allows you to define custom compression logic, integrate `compact-memory` into larger research pipelines, conduct detailed programmatic evaluations, and extend the library's capabilities.\n",
    "\n",
    "To use `compact-memory` with Python for these purposes (as shown in the following sections), we first need to import the necessary modules. With the packaging in the `scottfalconer/compact-memory` repository, imports should now use the standard package paths. Notably, `TokenBudget` has been removed from some interfaces due to earlier import issues; integer values are now used directly for token budget specifications in strategy methods and experiment configurations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml # For pretty-printing experiment results\n",
    "from pathlib import Path\n",
    "\n",
    "# Core compact-memory components for experiments and strategy evaluation\n",
    "from compact_memory import (\n",
    "    ExperimentConfig,\n",
    "    HistoryExperimentConfig,\n",
    "    ResponseExperimentConfig,\n",
    "    run_experiment,\n",
    "    run_history_experiment,\n",
    "    run_response_experiment,\n",
    "    StrategyConfig # Now directly available or re-exported\n",
    ")\n",
    "\n",
    "# Components from submodules\n",
    "from compact_memory.compression import (\n",
    "    CompressionStrategy,\n",
    "    CompressedMemory,\n",
    "    CompressionTrace\n",
    "    # TokenBudget removed due to ImportError\n",
    ")\n",
    "from compact_memory.schema import TextBlock\n",
    "\n",
    "from compact_memory.constants import ONBOARDING_DEMO_DIR \n",
    "from compact_memory.registry import register_compression_strategy \n",
    "from compact_memory.utils import read_text, setup_logging \n",
    "\n",
    "# Initialize logging for cleaner output\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Defining and Using Custom Compression Strategies in Python\n",
    "\n",
    "While the CLI is useful for applying pre-built strategies, the Python interface offers the power to define entirely new compression logic or precisely configure existing ones. At the heart of this is the `CompressionStrategy` class. This section demonstrates how to create and use your own custom strategy, giving you full control over the compression process."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.1 Define a Custom Compression Strategy\n",
    "\n",
    "We'll create `TruncateStrategy`, a basic strategy that truncates text to a specified token budget. This demonstrates the fundamental structure of a `CompressionStrategy`:\n",
    "- It inherits from `CompressionStrategy`.\n",
    "- It has a unique `id`.\n",
    "- It implements the `compress` method, which takes a `TextBlock` and an integer token budget, and returns a `CompressedMemory` object and a `CompressionTrace` object."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncateStrategy(CompressionStrategy):\n",
    "    \"\"\"A simple strategy that truncates text to the token budget.\"\"\"\n",
    "    id = \"truncate\" # Unique identifier for this strategy\n",
    "\n",
    "    def __init__(self, config: StrategyConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def compress(self, text_block: TextBlock, budget: int) -> tuple[CompressedMemory, CompressionTrace]: # budget is now int\n",
    "        \"\"\"Compresses text by truncating to the budget.\"\"\"\n",
    "        input_summary = self.summarize(text_block) # Get summary of input\n",
    "        \n",
    "        # Simple truncation based on tokens\n",
    "        tokens = self.tokenizer.encode(text_block.text)\n",
    "        # budget is already an int, no need for budget.value\n",
    "        truncated_tokens = tokens[:budget] # Slice tokens to meet budget \n",
    "        compressed_text = self.tokenizer.decode(truncated_tokens)\n",
    "        \n",
    "        # Create the CompressedMemory object\n",
    "        compressed_memory = CompressedMemory(\n",
    "            text=compressed_text,\n",
    "            interaction_id=text_block.interaction_id, # Preserve interaction context\n",
    "            source_block_ids=[text_block.id] # Track original source\n",
    "        )\n",
    "        output_summary = self.summarize(compressed_memory.to_text_block()) # Get summary of output\n",
    "        \n",
    "        # Create the CompressionTrace object for logging and analysis\n",
    "        # Assuming self.config is a Pydantic model (StrategyConfig)\n",
    "        strategy_config_dump = self.config.model_dump() if hasattr(self.config, 'model_dump') else self.config\n",
    "        trace = CompressionTrace(\n",
    "            strategy_name=self.id,\n",
    "            strategy_config=strategy_config_dump, \n",
    "            token_budget=budget, # Pass the integer budget directly\n",
    "            input_summary=input_summary,\n",
    "            output_summary=output_summary,\n",
    "            details={\"truncation_details\": \"Tokens truncated from start\"} # Strategy-specific details\n",
    "        )\n",
    "        return compressed_memory, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.2 Register the Custom Strategy\n",
    "\n",
    "To make our `TruncateStrategy` available for use by its ID (e.g., in experiments), we register it with the library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_compression_strategy(TruncateStrategy.id, TruncateStrategy)\n",
    "print(f\"Strategy '{TruncateStrategy.id}' registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.3 Load Sample Data\n",
    "\n",
    "Let's load a sample text file. Since we've changed directory into `compact-memory`, paths are now relative to the repository root. We'll use an excerpt about the moon landing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the sample data file (relative to repo root).\n",
    "sample_data_file_path = Path('sample_data/moon_landing/01_landing.txt')\n",
    "\n",
    "# Load the content of the file\n",
    "if sample_data_file_path.exists():\n",
    "    sample_text = read_text(sample_data_file_path)\n",
    "    print(f\"Successfully loaded data from {sample_data_file_path}\\n\")\n",
    "    print(\"First 300 characters of the sample data:\\n\")\n",
    "    print(sample_text[:300])\n",
    "else:\n",
    "    print(f\"Error: Sample data file not found at {sample_data_file_path}.\")\n",
    "    print(f\"Current directory: {Path.cwd()}\")\n",
    "    # Fallback text if file not found, to allow notebook to continue\n",
    "    sample_text = (\"This is a fallback text because the sample data file was not found. \"\n",
    "                   \"Please check the path. This text will be used for demonstration purposes instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.4 Demonstrate Text Compression\n",
    "\n",
    "Now, we'll use our `TruncateStrategy` to compress the loaded sample text. We need to:\n",
    "1.  Create a `StrategyConfig` instance.\n",
    "2.  Instantiate our `TruncateStrategy` with this config.\n",
    "3.  Create a `TextBlock` from our sample text.\n",
    "4.  Define an integer token budget.\n",
    "5.  Call the `compress` method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a StrategyConfig for TruncateStrategy.\n",
    "truncate_strategy_config_obj = StrategyConfig(name=\"truncate_demo_config\", tokenizer_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Create an instance of the TruncateStrategy\n",
    "truncate_compressor = TruncateStrategy(config=truncate_strategy_config_obj)\n",
    "\n",
    "# 3. Define a TextBlock for compression\n",
    "# TextBlock wraps the input text and associated metadata.\n",
    "text_to_compress_block = TextBlock(id=\"moon_landing_excerpt\", text=sample_text, interaction_id=\"demo_interaction_01\")\n",
    "\n",
    "# 4. Define an integer token budget (e.g., compress to a maximum of 75 tokens)\n",
    "compression_budget_int = 75 \n",
    "\n",
    "# 5. Compress the text\n",
    "compressed_memory_output, compression_trace_output = truncate_compressor.compress(text_to_compress_block, compression_budget_int)\n",
    "\n",
    "print(f\"Compression complete using strategy '{truncate_compressor.id}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2.5 Understanding `CompressedMemory` and `CompressionTrace`\n",
    "\n",
    "The `compress` method returned two important objects:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### `CompressedMemory`\n",
    "\n",
    "The `CompressedMemory` object encapsulates the result of the compression. Its most important attribute is `text`, which contains the compressed version of the input. It also stores metadata linking it to the original text and the interaction it belongs to."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Compressed Text Output ---:\\n\")\n",
    "print(compressed_memory_output.text)\n",
    "print(f\"\\nOriginal Text Length (tokens): {compression_trace_output.input_summary.num_tokens}\")\n",
    "print(f\"Compressed Text Length (tokens): {compression_trace_output.output_summary.num_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### `CompressionTrace`\n",
    "\n",
    "The `CompressionTrace` object logs detailed metadata about the compression process. This is invaluable for debugging, analysis, and understanding the behavior of a strategy. It includes:\n",
    "- The strategy name and its configuration.\n",
    "- The token budget applied.\n",
    "- Summaries of the input and output text (character counts, word counts, sentence counts, token counts).\n",
    "- Any custom details logged by the strategy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Compression Trace Information ---:\\n\")\n",
    "print(f\"Strategy Name: {compression_trace_output.strategy_name}\")\n",
    "print(f\"Strategy Configuration: {compression_trace_output.strategy_config}\")\n",
    "# Assuming CompressionTrace.token_budget can now store an int or was adapted\n",
    "print(f\"Token Budget: {compression_trace_output.token_budget} tokens\") \n",
    "print(f\"Input Summary: Chars={compression_trace_output.input_summary.num_chars}, Words={compression_trace_output.input_summary.num_words}, Sents={compression_trace_output.input_summary.num_sents}, Tokens={compression_trace_output.input_summary.num_tokens}\")\n",
    "print(f\"Output Summary: Chars={compression_trace_output.output_summary.num_chars}, Words={compression_trace_output.output_summary.num_words}, Sents={compression_trace_output.output_summary.num_sents}, Tokens={compression_trace_output.output_summary.num_tokens}\")\n",
    "print(f\"Strategy-Specific Details: {compression_trace_output.details}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Evaluating Strategies and Testing Results in Python\n",
    "\n",
    "Beyond applying strategies, it's crucial to evaluate their effectiveness. `compact-memory` provides a powerful experiment framework for systematically testing your custom strategies or different configurations of built-in ones. This allows you to gather metrics and compare performance for tasks like document ingestion, history retrieval, and response generation. This section explains how to set up and run these experiments.\n",
    "\n",
    "Researchers can extend this framework by adding new experiment types or custom evaluation metrics relevant to their specific research questions. The existing `run_experiment`, `run_history_experiment`, and `run_response_experiment` functions provide a solid foundation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Experiment Configuration Objects:\n",
    "-   **`ExperimentConfig`**: Used for *ingestion experiments*. These evaluate how well a strategy compresses a single, potentially large, text document. Key metrics often include compression ratio, information preservation (if measurable via summarization or other techniques), and processing time.\n",
    "-   **`HistoryExperimentConfig`**: Used for *history retrieval experiments*. These assess a strategy's ability to compress dialogue history while retaining information that is useful for retrieving relevant past turns or facts. Metrics typically involve retrieval scores like Mean Reciprocal Rank (MRR) or Recall@k against ground-truth relevant items.\n",
    "-   **`ResponseExperimentConfig`**: Used for *response generation experiments*. These evaluate the quality of LLM-generated responses when the compressed dialogue history (produced by the strategy) is used as context. Evaluation can involve automated metrics (e.g., ROUGE, BLEU for summarization/translation tasks) or human evaluation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3.1 Run Ingestion Experiment\n",
    "\n",
    "This experiment type evaluates how well a strategy compresses a single document. We'll use the `TruncateStrategy` and our moon landing text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data for the ingestion experiment (reusing the sample data path, now relative to repo root)\n",
    "ingestion_data_path = sample_data_file_path # Path('sample_data/moon_landing/01_landing.txt')\n",
    "\n",
    "if ingestion_data_path.exists():\n",
    "    # Create an ExperimentConfig for ingestion\n",
    "    ingestion_exp_config = ExperimentConfig(\n",
    "        id=\"ingestion_demo_experiment\",\n",
    "        experiment_type=\"ingestion\",\n",
    "        strategy_id=TruncateStrategy.id, \n",
    "        strategy_config=truncate_strategy_config_obj, \n",
    "        dataset_path=str(ingestion_data_path), \n",
    "        token_budgets=[50, 100], # Integer token budgets\n",
    "        output_dir=\"./ingestion_experiment_output\" \n",
    "    )\n",
    "\n",
    "    # Run the ingestion experiment\n",
    "    print(f\"Running ingestion experiment: {ingestion_exp_config.id}\")\n",
    "    ingestion_metrics_results = run_experiment(ingestion_exp_config)\n",
    "\n",
    "    # Print the metrics in a readable YAML format\n",
    "    print(\"\\n--- Ingestion Experiment Metrics ---:\\n\")\n",
    "    print(yaml.safe_dump(ingestion_metrics_results, indent=2, sort_keys=False))\n",
    "else:\n",
    "    print(f\"Error: Ingestion data file not found at {ingestion_data_path}. Current CWD: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output above typically includes metrics like compression ratio, number of tokens before and after compression for each budget, and processing time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3.2 Run History Experiment\n",
    "\n",
    "This experiment evaluates a strategy's ability to compress dialogue history for effective retrieval. It uses a dataset of dialogues where specific past turns are marked as relevant to current turns.\n",
    "\n",
    "**Note**: The `truncate` strategy is very naive for history retrieval. More sophisticated strategies (like those using summarization or embedding similarity) would likely perform better here. This is for demonstration of the experiment mechanics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the history dialogues data (relative to repo root)\n",
    "history_dialogues_data_path = Path('tests/data/history_dialogues.yaml')\n",
    "\n",
    "if history_dialogues_data_path.exists():\n",
    "    # Create a HistoryExperimentConfig\n",
    "    history_exp_config = HistoryExperimentConfig(\n",
    "        id=\"history_demo_experiment\",\n",
    "        experiment_type=\"history_retrieval\",\n",
    "        strategy_id=TruncateStrategy.id,\n",
    "        strategy_config=truncate_strategy_config_obj, \n",
    "        dataset_path=str(history_dialogues_data_path),\n",
    "        # param_grid allows testing different strategy parameters or budgets\n",
    "        # Using integer for token_budget as TokenBudget class is not imported\n",
    "        param_grid={\"token_budget\": [100, 150]},\n",
    "        output_dir=\"./history_experiment_output\"\n",
    "    )\n",
    "\n",
    "    # Run the history experiment\n",
    "    print(f\"Running history experiment: {history_exp_config.id}\")\n",
    "    history_experiment_results = run_history_experiment(history_exp_config)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\n--- History Experiment Results ---:\\n\")\n",
    "    print(yaml.safe_dump(history_experiment_results, indent=2, sort_keys=False))\n",
    "else:\n",
    "    print(f\"Error: History dialogues data not found at {history_dialogues_data_path}. Current CWD: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results from a history experiment usually include retrieval metrics like MRR (Mean Reciprocal Rank) and Recall@k, indicating how well the compressed history helped in identifying relevant prior turns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3.3 Run Response Experiment\n",
    "\n",
    "A response experiment assesses how compressed memory affects the quality of responses from an LLM. It uses a dataset of dialogues where the task is to generate a response based on the history.\n",
    "\n",
    "**Note**: This experiment can take longer to run as it involves calls to an LLM (even a small local one like `tiny-gpt2`). The `truncate` strategy's impact on response quality might be detrimental if too much context is lost."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the response dialogues data (relative to repo root)\n",
    "response_dialogues_data_path = Path('tests/data/response_dialogues.yaml')\n",
    "\n",
    "if response_dialogues_data_path.exists():\n",
    "    # Create a ResponseExperimentConfig\n",
    "    response_exp_config = ResponseExperimentConfig(\n",
    "        id=\"response_demo_experiment\",\n",
    "        experiment_type=\"response_generation\",\n",
    "        strategy_id=TruncateStrategy.id, \n",
    "        strategy_config=truncate_strategy_config_obj, \n",
    "        dataset_path=str(response_dialogues_data_path),\n",
    "        # Using integer for token_budget\n",
    "        param_grid={\"token_budget\": [100]},\n",
    "        chat_model_name=\"gpt2\", # LLM for generating responses\n",
    "        output_dir=\"./response_experiment_output\"\n",
    "    )\n",
    "\n",
    "    # Run the response experiment\n",
    "    print(f\"Running response experiment: {response_exp_config.id}\")\n",
    "    # This can take a few minutes depending on the dataset size and model\n",
    "    response_experiment_results = run_response_experiment(response_exp_config)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\n--- Response Experiment Results ---:\\n\")\n",
    "    print(yaml.safe_dump(response_experiment_results, indent=2, sort_keys=False))\n",
    "else:\n",
    "    print(f\"Error: Response dialogues data not found at {response_dialogues_data_path}. Current CWD: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Response experiment results often include metrics like ROUGE or BLEU (if reference responses are available and the task is suitable), or qualitative examples of generated responses. The output here will show file paths where detailed per-instance results and generated texts are stored."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion and Research Opportunities\n",
    "\n",
    "This notebook has provided a researcher-focused guide to the `scottfalconer/compact-memory` fork, covering:\n",
    "-   **Installation and Setup**: Ensuring you can get the library up and running.\n",
    "-   **Command Line Interface (CLI)**: Using the CLI for preliminary experiments and data handling.\n",
    "-   **Advanced Python Interface**: Delving into custom strategy development and the experiment framework for rigorous evaluation of compression techniques.\n",
    "\n",
    "`compact-memory` offers a flexible platform for research into text compression for LLMs. Key areas for future research and contribution include:\n",
    "-   **Developing Novel Compression Strategies**: Explore new algorithms beyond simple truncation or gist-based summarization. This could involve neural methods, reinforcement learning, or hybrid approaches.\n",
    "-   **Creating Advanced Evaluation Metrics**: Current metrics evaluate aspects like compression ratio and retrieval accuracy. New metrics could focus on semantic fidelity, factual consistency, or downstream task performance after compression.\n",
    "-   **Designing New Experiment Types**: The existing experiment configurations (ingestion, history, response) can be extended. For example, one might design experiments for few-shot learning with compressed context, or for multi-document summarization.\n",
    "-   **Improving Tokenization and Budgeting**: Research more nuanced ways to define and apply token budgets, potentially varying budgets dynamically or based on content characteristics.\n",
    "-   **Extending the Plugin System**: Contribute to making the library more extensible, for example, by enhancing the plugin system for strategies or evaluation components.\n",
    "\n",
    "### Further Resources for Researchers:\n",
    "-   **GitHub Repository**: [https://github.com/scottfalconer/compact-memory](https://github.com/scottfalconer/compact-memory)\n",
    "-   **Documentation (`docs/` directory)**: Explore the `docs/` directory within the repository for more detailed information on the library's architecture, contributing guidelines, and API specifications. This is crucial for extending the library.\n",
    "-   **Academic Papers**: Search for papers citing `compact-memory` or related work in LLM context management and text compression to understand the current research landscape.\n",
    "\n",
    "We encourage you to use `compact-memory` in your research, contribute your findings, and help advance the field of efficient LLM context management. Feel free to open issues or pull requests on the GitHub repositories."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "compact_memory_researcher_guide.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

[end of examples/compact_memory_researcher_guide.ipynb]
