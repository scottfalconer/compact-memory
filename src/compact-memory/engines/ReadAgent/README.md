# ReadAgentGistEngine for Compact Memory

**Engine ID:** `readagent_gist`

The `ReadAgentGistEngine` is a contrib module for the `compact-memory` library, designed to implement a memory compression strategy inspired by the ReadAgent architecture. This approach focuses on creating "gists" (concise summaries) of text episodes and using these gists for efficient long-context processing, summarization, and question answering.

## Overview

The core idea behind this engine is to mimic a human's process of reading and understanding long documents:
1.  **Episode Pagination:** The input text is first divided into smaller, manageable "episodes" or "pages."
2.  **Memory Gisting:** Each episode is then summarized by a local Large Language Model (LLM) into a short "gist." This gist captures the essence of the episode.
3.  **Gist-Based Processing:**
    *   **For Summarization:** The gists of all episodes are concatenated. If this combined text is still too long for a target budget, it can be further summarized by the LLM.
    *   **For Question Answering:** When a question is posed, the LLM first reviews all the gists to determine which original episodes are most likely to contain the answer. It then "looks up" the full text of these selected episodes. The final answer is generated by the LLM using a context composed of the full text of relevant episodes and the gists of the remaining episodes.

This engine is designed for **offline use**, meaning it relies on a user-provided local LLM pipeline for all summarization and question-answering tasks.

## Key Features

*   **Episodic Gisting:** Breaks down long texts and creates concise summaries (gists) for each part.
*   **Interactive Lookup for QA:** Intelligently selects relevant full-text episodes for answering questions, based on initial gist summaries.
*   **Summarization:** Can produce summaries of long documents by first creating gists and then summarizing the collection of gists.
*   **Extensible:** Designed to be configured with different local LLM pipelines and prompt templates.
*   **Traceability:** Provides detailed `CompressionTrace` objects to inspect the internal steps and decisions made by the engine.
*   **Persistence:** Leverages the base engine's `ingest` and `recall` mechanisms to store and retrieve gists, allowing for pre-processing of documents.

## Usage

The `ReadAgentGistEngine` is instantiated with a configuration that includes a callable local LLM pipeline and various prompt templates.

```python
from compact_memory.engines.ReadAgent.engine import ReadAgentGistEngine
# Assume 'my_local_llm_summarizer_pipeline' is a function you have that takes a prompt and returns text.

# Example: Define a simple LLM pipeline function (replace with your actual LLM call)
def my_llm_func(prompt: str) -> str:
    if "Summarize:" in prompt:
        return f"Simulated summary for: {prompt[:50]}..."
    elif "Relevant pages:" in prompt:
        return "Page 1" # Simulate selecting the first page
    elif "Context:" in prompt and "Question:" in prompt:
        return f"Simulated answer for: {prompt.split('Question:')[1][:50]}..."
    return "LLM response."

engine_config = {
    "local_llm_pipeline": my_llm_func,
    # Other parameters like gist_length, prompt templates can be set here
}

read_agent_engine = ReadAgentGistEngine(**engine_config)

# Summarization
long_document = "This is the first episode.\n\nThis is the second episode, much longer.\n\nAnd a third."
compressed_summary, trace_summary = read_agent_engine.compress(
    long_document,
    llm_token_budget=100 # Target character length for the final summary
)
print(f"Summary: {compressed_summary.text}")

# Question Answering
question = "What is in the first episode?"
compressed_answer, trace_qa = read_agent_engine.compress(
    long_document,
    llm_token_budget=50, # Target character length for the answer
    query=question
)
print(f"Answer: {compressed_answer.text}")

# Ingesting gists (for later recall)
# This will use the configured chunker to split `long_document` into "episodes",
# then our `_generate_gist` (via `_compress_chunk`) will create gists for each.
# These gists and their embeddings are stored by the base engine.
# Make sure to configure an embedding_fn for the engine if using recall.
# e.g., read_agent_engine.embedding_fn = some_embedding_function
# ids = read_agent_engine.ingest(long_document)
# retrieved_gists = read_agent_engine.recall(query="first episode content", top_k=1)
# if retrieved_gists:
#     print(f"Recalled gist: {retrieved_gists[0]['text']}")

```

## Configuration Parameters

When initializing `ReadAgentGistEngine`, the following parameters can be provided (often via the `config` dictionary):

*   `local_llm_pipeline`: (Required) A callable function or object that takes a prompt string and returns the LLM's generated text.
*   `chunker`: (Optional) A `Chunker` instance from `compact_memory.chunker`. Defaults to `SentenceWindowChunker`. This is used by the `ingest` method to break text into episodes before gisting.
*   `embedding_fn`: (Optional) An embedding function used by `ingest` and `recall`.
*   `episode_token_limit`: (Optional, int) A target token limit for episodes (currently used as a guideline, actual pagination is by `_paginate_episodes`). Default: 500.
*   `gist_length`: (Optional, int) Target token length for each gist summary. Default: 100.
*   `gist_prompt_template`: (Optional, str) Prompt template for generating gists. Default: `"Summarize the following text in about {gist_length} tokens: {text}"`.
*   `qa_prompt_template`: (Optional, str) Prompt template for generating the final answer in QA. Default: `"Based on the following context, answer the question. Context: {context} Question: {question}"`.
*   `lookup_prompt_template`: (Optional, str) Prompt template for selecting relevant episodes based on gists for QA. Default: `"Based on the following summaries of pages and the question, which page(s) likely contain the details needed? Question: {question} Summaries: {summaries}"`.

## Inspired By

This engine's architecture is inspired by the concepts presented in the ReadAgent paper:
*   Lee, K., Ippolito, D., Nystrom, A., Liahov, G., Cohen, G., Laskin, A., ... & Collins, M. (2024). ReadAgent: Scaling Long-context Transformers by Reading Selective Gists. *arXiv preprint arXiv:2407.08779*. Also available at: [https://deepmind.google/research/publications/74917/](https://deepmind.google/research/publications/74917/)

Further development can enhance episode pagination, summarization strategies, and lookup mechanisms based on the detailed methods described in the paper.
