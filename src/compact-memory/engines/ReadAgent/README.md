# ReadAgentGistEngine for Compact Memory

**Engine ID:** `readagent_gist`

The `ReadAgentGistEngine` is a contrib module for the `compact-memory` library, designed to implement a memory compression strategy inspired by the ReadAgent architecture. This approach focuses on creating "gists" (concise summaries) of text episodes and using these gists for efficient long-context processing, summarization, and question answering.

## Overview

The core idea behind this engine is to mimic a human's process of reading and understanding long documents:
1.  **Episode Pagination:** The input text is first divided into smaller, manageable "episodes" or "pages."
2.  **Memory Gisting:** Each episode is then summarized by a Large Language Model (LLM) into a short "gist." This gist captures the essence of the episode. This is done via an `LLMProvider` interface.
3.  **Gist-Based Processing:**
    *   **For Summarization:** The gists of all episodes are concatenated. If this combined text is still too long for a target budget, it can be further processed (though current implementation simply truncates).
    *   **For Question Answering:** When a question is posed, the LLM (via `LLMProvider`) first reviews all the gists to determine which original episodes are most likely to contain the answer. It then "looks up" the full text of these selected episodes. The final answer is generated by the LLM using a context composed of the full text of relevant episodes and the gists of the remaining episodes.

This engine is designed to be flexible, allowing users to plug in various LLM backends through the `LLMProvider` interface. If no provider is specified, it falls back to simulated responses.

## Key Features

*   **Episodic Gisting:** Breaks down long texts and creates concise summaries (gists) for each part using a configured `LLMProvider`.
*   **Interactive Lookup for QA:** Intelligently selects relevant full-text episodes for answering questions, based on initial gist summaries, orchestrated via the `LLMProvider`.
*   **Summarization:** Can produce summaries of long documents by first creating gists and then summarizing the collection of gists.
*   **Pluggable LLM Providers:** Supports different LLM backends (`LocalTransformersProvider`, `OpenAIProvider`, `GeminiProvider`, `MockLLMProvider`) via the `llm_provider` parameter.
*   **Simulation Mode:** If `llm_provider` is `None`, the engine simulates LLM responses, useful for testing or running without an active LLM.
*   **Extensible:** Designed to be configured with different prompt templates and model settings.
*   **Traceability:** Provides detailed `CompressionTrace` objects to inspect the internal steps and decisions made by the engine.
*   **Persistence:** Leverages the base engine's `ingest` and `recall` mechanisms to store and retrieve gists, allowing for pre-processing of documents.

## Usage

The `ReadAgentGistEngine` is instantiated with an `LLMProvider` and an optional configuration dictionary.

```python
from compact_memory.engines.ReadAgent.engine import ReadAgentGistEngine
from compact_memory.llm_providers import MockLLMProvider, LocalTransformersProvider
# For other providers:
# from compact_memory.llm_providers import OpenAIProvider, GeminiProvider

# Option 1: Using MockLLMProvider (for testing or predefined behavior)
mock_provider = MockLLMProvider()
# You can add expected responses to the mock_provider if needed:
# mock_provider.add_response("Summarize this text in about 30 tokens: Test episode", "Mocked gist for test episode")

engine_with_mock = ReadAgentGistEngine(
    llm_provider=mock_provider,
    config={
        "gist_length": 30, # Corresponds to max_new_tokens for gisting
        # Other parameters like prompt templates, model names can be set here
    }
)
summary_mock, _ = engine_with_mock.compress("Test episode", llm_token_budget=50)
print(f"Summary (Mock): {summary_mock.text}")


# Option 2: Using LocalTransformersProvider (requires a local model)
# Note: Ensure you have transformers, torch, and a suitable model installed.
# By default, LocalTransformersProvider tries to use 'distilgpt2'.
# You might need to specify a model_name if 'distilgpt2' is not appropriate for gisting/QA.
try:
    local_provider = LocalTransformersProvider() # Uses LocalChatModel by default
    engine_with_local = ReadAgentGistEngine(
        llm_provider=local_provider,
        config={
            "gist_length": 50,
            "gist_model_name": "distilgpt2", # Example, choose as appropriate
            "lookup_model_name": "distilgpt2",
            "qa_model_name": "distilgpt2",
            "qa_max_new_tokens": 100, # Max tokens for the QA answer
        }
    )
    # This call would use the actual local LLM
    # summary_local, _ = engine_with_local.compress("A real document to process.", llm_token_budget=100)
    # print(f"Summary (Local LLM): {summary_local.text}")
except ImportError:
    print("LocalTransformersProvider requires 'transformers' and 'torch'. Skipping example.")
except Exception as e:
    print(f"Error setting up LocalTransformersProvider (likely model download/access issue): {e}")


# Option 3: Using OpenAIProvider (requires OPENAI_API_KEY environment variable)
# from compact_memory.llm_providers import OpenAIProvider
# try:
#     openai_provider = OpenAIProvider()
#     engine_with_openai = ReadAgentGistEngine(
#         llm_provider=openai_provider,
#         config={
#             "gist_length": 50,
#             "gist_model_name": "gpt-3.5-turbo-instruct", # Or another suitable completion model
#             "lookup_model_name": "gpt-3.5-turbo-instruct",
#             "qa_model_name": "gpt-3.5-turbo-instruct",
#         }
#     )
#     # summary_openai, _ = engine_with_openai.compress("OpenAI powered document processing.", llm_token_budget=100)
#     # print(f"Summary (OpenAI): {summary_openai.text}")
# except ImportError:
#     print("OpenAIProvider requires 'openai'. Skipping example.")
# except Exception as e:
#     print(f"Error setting up OpenAIProvider (check API key and connectivity): {e}")


# If llm_provider is None, it simulates responses:
engine_simulated = ReadAgentGistEngine(llm_provider=None, config={"gist_length": 30})
long_document = "This is the first episode.\n\nThis is the second episode, much longer.\n\nAnd a third."
compressed_summary, trace_summary = engine_simulated.compress(
    long_document,
    llm_token_budget=100 # Target character length for the final summary (used for truncation)
)
print(f"Summary (Simulated): {compressed_summary.text}")

question = "What is in the first episode?"
compressed_answer, trace_qa = engine_simulated.compress(
    long_document,
    llm_token_budget=50, # Target character length for the answer
    query=question
)
print(f"Answer (Simulated): {compressed_answer.text}")

```

## Configuration Parameters

When initializing `ReadAgentGistEngine`, the `llm_provider` is the primary way to configure LLM interaction. The `config` dictionary can hold additional parameters:

*   `llm_provider`: (Optional) An instance of a class that implements `LLMProvider` (e.g., `LocalTransformersProvider`, `OpenAIProvider`, `GeminiProvider`, `MockLLMProvider`). If `None`, the engine simulates LLM responses.
*   `chunker`: (Optional) A `Chunker` instance from `compact_memory.chunker`. Defaults to `SentenceWindowChunker`. Used by `ingest` method.
*   `embedding_fn`: (Optional) An embedding function used by `ingest` and `recall`.
*   `episode_token_limit`: (Optional, int) A guideline for episode length (token count). Actual pagination is by `_paginate_episodes` (newline based). Default: 500.
*   **LLM Interaction Parameters (via `config`):**
    *   `gist_length`: (Optional, int) Target **max new tokens** for each gist summary generated by the LLM. Default: 100.
    *   `gist_model_name`: (Optional, str) Model name to be used by the `llm_provider` for generating gists. Default: "distilgpt2" (but should be chosen based on the provider).
    *   `lookup_model_name`: (Optional, str) Model name for selecting relevant episodes. Default: "distilgpt2".
    *   `lookup_max_tokens`: (Optional, int) Max new tokens for the episode selection LLM call. Default: 50.
    *   `qa_model_name`: (Optional, str) Model name for generating the final answer in QA. Default: "distilgpt2".
    *   `qa_max_new_tokens`: (Optional, int) Max new tokens for the QA answer generation. Default: 250.
*   **Prompt Templates (via `config`):**
    *   `gist_prompt_template`: (Optional, str) Default: `"Summarize the following text in about {gist_length} tokens: {text}"`.
    *   `qa_prompt_template`: (Optional, str) Default: `"Based on the following context, answer the question. Context: {context} Question: {question}"`.
    *   `lookup_prompt_template`: (Optional, str) Default: `"Based on the following summaries of pages and the question, which page(s) likely contain the details needed? Question: {question} Summaries: {summaries}"`.

## Inspired By

This engine's architecture is inspired by the concepts presented in the ReadAgent paper:
*   Lee, K., Ippolito, D., Nystrom, A., Liahov, G., Cohen, G., Laskin, A., ... & Collins, M. (2024). ReadAgent: Scaling Long-context Transformers by Reading Selective Gists. *arXiv preprint arXiv:2407.08779*. Also available at: [https://deepmind.google/research/publications/74917/](https://deepmind.google/research/publications/74917/)

Further development can enhance episode pagination, summarization strategies, and lookup mechanisms based on the detailed methods described in the paper.
