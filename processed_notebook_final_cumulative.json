{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Compact Memory User Guide\n",
    "\n",
    "Welcome to the  user guide! This notebook provides a hands-on introduction to the  library, focusing on its command-line interface.\n",
    "\n",
    " helps you manage and compress text data (like dialogue history or large documents) to fit within the limited context windows of Large Language Models (LLMs) while retaining essential information. This guide focuses on using the  library.\n",
    "\n",
    "This notebook will walk you through:\n",
    "1.  **Setup**: Cloning the  repository, changing into its directory, and installing the package along with dependencies.\n",
    "2.  **Command Line Interface (CLI)**: A tour of the  CLI for compressing text, trying different strategies, and interacting with an LLM.\n",
    "3.  **Conclusion**: Summary and next steps for using .\n",
    "\n",
    "**Important Note on Notebook Execution**: This notebook uses  to change the current directory. Subsequent cells will operate from within the cloned  repository root.\n",
    "\n",
    "Let's get started!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Initial Setup: Install Dependencies\n",
    "\n",
    "First, we need to set up the environment. This involves cloning the  repository, changing our current directory into it, installing the package from local source, and then downloading necessary dependencies and models.\n",
    "\n",
    "**Note**: If you are running this notebook locally and have already cloned this repository and set up a virtual environment from within the repo root, you might be able to skip some of these steps. However, these cells are designed to work in a fresh Google Colab environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Clone `compact-memory` Repository and Change Directory"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the compact-memory repository\n",
    "!git clone https://github.com/scottfalconer/compact-memory.git\n",
    "\n",
    "# Change directory into the cloned repository\n",
    "# Subsequent commands will run from the root of the 'compact-memory' repository\n",
    "%cd compact-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we are inside the `compact-memory` directory (from the `scottfalconer/compact-memory` repository), we can install the package and its dependencies using relative paths."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Install `compact-memory` Package from Local Source"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the compact-memory package from the local source (current directory)\n",
    "# The --no-build-isolation flag can be helpful if there are issues with \n",
    "# the build environment or specific package versions.\n",
    "!pip install . --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Download spaCy Model\n",
    "\n",
    "Download the English language model from spaCy, which is used for text processing tasks like sentence segmentation. SpaCy should have been installed as part of the `requirements.txt`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.5 Set up PYTHONPATH (Usually Not Needed with `pip install .`)\n",
    "\n",
    "When a package is installed using `pip install .` from its root directory, it's typically placed in the Python environment's `site-packages` directory and becomes accessible. Explicitly modifying `sys.path` is usually not necessary.\n",
    "\n",
    "The following cell is commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.getcwd()) # os.getcwd() is now the repo root, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.6 Download Pre-trained Models for `compact-memory`\n",
    "\n",
    "Download the default embedding model (for text vectorization) and a small chat model (for response generation experiments) using the `compact-memory` CLI. The CLI should be available after the `pip install .` step."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the default sentence transformer model for embeddings\n",
    "!compact-memory dev download-embedding-model --model-name all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small chat model for demonstration purposes\n",
    "!compact-memory dev download-chat-model --model-name gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.7 Configure Offline Usage (Optional)\n",
    "\n",
    "If you have downloaded all necessary models and want to ensure the notebook runs without attempting to access Hugging Face Hub, you can set these environment variables. For this showcase, we'll leave them commented out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For offline use after all models are downloaded, uncomment the following lines:\n",
    "# import os\n",
    "# os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "# os.environ['TRANSFORMERS_OFFLINE'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Command Line Interface (CLI) Showcase\n",
    "\n",
    "`compact-memory` also features a versatile Command Line Interface (CLI) for performing common operations without needing to write Python scripts. This is handy for quick tests, batch processing, model downloads, and direct interaction with compression strategies. The CLI became available after we installed `compact-memory` using `pip`.\n",
    "\n",
    "Since we are operating from within the `compact-memory` repository root, file paths for the CLI should also be relative to this root."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Basic Help Command\n",
    "\n",
    "To view all available CLI commands and their general options, use the `--help` flag."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 Compression Command ()\n",
    "\n",
    "The  command lets you apply a compression engine to various forms of input: a direct line of text (using ), or a single file or an entire directory (by providing the path as a positional argument). The following subsections will demonstrate these use cases.\n",
    "\n",
    "The CLI uses strategies that are built into  (e.g., , , , ). For these examples, we'll use some of these built-in strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Compress a short text string using the 'prototype' engine\n",
    "# This command demonstrates compressing a simple line of text. \n",
    "# We use the 'prototype' engine, fitting it within a 20-token budget.\n",
    "!compact-memory compress --engine prototype --text \"This is a fairly long sentence that we want to compress using the command line interface to a small number of tokens.\" --budget 20\n",
    "\n",
    "# Example: Compress a text file using the 'prototype' engine\n",
    "# This command demonstrates compressing an entire file. We use 'sample_data/moon_landing/full.txt'.\n",
    "# The 'prototype' engine is employed to summarize the content within a 100-token budget.\n",
    "# To compress a file, provide its path as an argument using --input-file. For instance:\n",
    "!compact-memory compress --engine prototype --input-file sample_data/moon_landing/full.txt --budget 100"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compress a short text string using the 'truncate' engine (if available as a built-in/plugin)\n",
    "# This command demonstrates compressing a simple line of text. \n",
    "# We use the 'truncate' engine to keep the beginning of the text, fitting it within a 20-token budget.\n",
    "!compact-memory compress --engine truncate --text \"This is a fairly long sentence that we want to compress using the command line interface to a small number of tokens.\" --budget 20\n",
    "\n",
    "# Example: Compress a text file using the 'gist' engine\n",
    "# This command demonstrates compressing an entire file. We use 'sample_data/moon_landing/full.txt'.\n",
    "# The 'gist' engine is employed to summarize the content within a 100-token budget.\n",
    "# To compress a file, provide its path as an argument. For instance:\n",
    "!compact-memory compress --engine gist sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example: Compress an entire directory using the 'prototype' engine\n",
    "# This command demonstrates compressing all supported files within a directory.\n",
    "# We use the 'sample_data/moon_landing' directory.\n",
    "# The 'prototype' engine will be applied to each file.\n",
    "# The  command uses --input-dir for directory paths.\n",
    "!compact-memory compress --engine prototype --input-dir sample_data/moon_landing --budget 200\n",
    "# You might want to add --output some_directory_output.json if you want to inspect results later"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Choosing a Engine with \n",
    "\n",
    "The  option is key to controlling how  compresses your text. Different strategies have different behaviors:\n",
    "\n",
    "*   ****: Aims to create a compressed representation using a prototype-based approach. (Example used above).\n",
    "*   ****: Allows chaining multiple processing steps (more advanced and requires specific configuration not covered in this basic example).\n",
    "*   ****: Passes through the text without compression (useful for baseline or token counting).\n",
    "*   ****: A simple engine that typically keeps the first and last parts of the text.\n",
    "\n",
    "You can switch between them easily. For example, to process our  using  (no compression) instead of :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of using a different engine (none) on the full.txt file\n",
    "# To process a file, provide its path as an argument using --input-file. For instance:\n",
    "!compact-memory compress --engine none --input-file sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using a different engine (truncate) on the full.txt file\n",
    "# To compress a file, provide its path as an argument. For instance:\n",
    "!compact-memory compress --engine truncate sample_data/moon_landing/full.txt --budget 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This attempts a non-interactive query with a specified chat model.\n",
    "# The CLI might provide a single response or indicate it's for interactive use.\n",
    "!compact-memory talk --chat-model gpt2 --message \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Other Useful CLI Commands\n",
    "\n",
    "The `compact-memory` CLI offers other utilities. Here are a few, with examples of how to get their help messages:\n",
    "\n",
    "-   `download-model`, `download-chat-model`: We used these in the setup to fetch models (e.g., `all-MiniLM-L6-v2`, `tiny-gpt2`).\n",
    "-   `stats`: Provides statistics about datasets or memory files.\n",
    "-   `validate`: Validates configuration files or memory formats against the library's schemas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!compact-memory agent stats --help\n",
    "!echo \"\\n---------------------------------------\\n\" # Visual separator\n",
    "!compact-memory agent validate --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusion and Next Steps\n",
    "\n",
    "This notebook has provided a user-focused introduction to the `scottfalconer/compact-memory` fork, specifically covering:\n",
    "-   **Installation and Setup**: How to get the library ready for use.\n",
    "-   **Command Line Interface (CLI)**: Practical examples of using the CLI to compress text, try different strategies, and provide context to an LLM with the `talk` command.\n",
    "\n",
    "With `compact-memory`, you can effectively manage text data to fit within LLM context windows. This is particularly useful for tasks like:\n",
    "-   Summarizing long documents before feeding them to an LLM.\n",
    "-   Maintaining context in extended conversations with chatbots.\n",
    "-   Processing large amounts of text for information retrieval when interacting with LLMs.\n",
    "\n",
    "We encourage you to experiment with the CLI commands and strategies shown in this guide using your own data.\n",
    "\n",
    "### Further Resources:\n",
    "-   **GitHub Repository**: [https://github.com/scottfalconer/compact-memory](https://github.com/scottfalconer/compact-memory)\n",
    "-   **CLI Help**: Use `compact-memory --help`, `compact-memory compress --help`, and `compact-memory talk --help` in your terminal for detailed CLI options.\n",
    "-   **Examples Folder**: Check the `examples/` directory in the repository for any other user-oriented examples or showcases that may be added.\n",
    "\n",
    "Happy compressing!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "compact_memory_user_guide.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}